{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective:\n",
    "- Deconstruct the essential components of llama-index to understand the important components\n",
    "- Log interaction into a db and csv file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import fitz ## PyMuPDF\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "from tqdm.notebook import tqdm\n",
    "import traceback\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Optional, Dict, Any\n",
    "import logging\n",
    "from rich import print\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_path=\"../../project_secrets.env\")\n",
    "load_dotenv(dotenv_path=\"../../../ai_sdlc_secrets.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Major Components of LlamaIndex:\n",
    "1. Document & Nodes - Represent and chunk text data\n",
    "2. Data Connectors - Load data from various sources\n",
    "3. Indexes - Store and organize the processed document data\n",
    "4. Vector Stores -  Manage embeddings and vector similarity search\n",
    "5. Retrievers - Extract relevant context from index\n",
    "6. Query Engines - Process queries and generate responses.\n",
    "7. Response Synthesizers - Combine retrieved context and query to generate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Document and Nodes\n",
    "class Document:\n",
    "    def __init__(self, text: str, metadata: Optional[Dict[str, Any]] = None):\n",
    "        self.text = text\n",
    "        self.metadata = metadata or {}\n",
    "\n",
    "class Node:\n",
    "    def __init__(\n",
    "            self, \n",
    "            text: str, \n",
    "            metadata: Optional[Dict[str, Any]] = None,\n",
    "            node_id: Optional[str] = None):\n",
    "        self.text = text\n",
    "        self.metadata = metadata or {}\n",
    "        self.node_id = node_id or f\"node_{id(self)}\" ## Simple Unique ID if not provided\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Node(id={self.node_id}, text={self.text[:50]}, metadata={self.metadata})\"\n",
    "    \n",
    "## 3. For Indexing      \n",
    "class SimpleNodeParser:\n",
    "    def __init__(self, chunk_size: int = 4096, chunk_overlap: int= 200) -> None:\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into chunks with overalap\"\"\"\n",
    "        if len(text) <= self.chunk_size:\n",
    "            return [text]\n",
    "        \n",
    "        chunks = []\n",
    "        for i in range(0, len(text), self.chunk_size - self.chunk_overlap):\n",
    "            chunk = text[i:i+self.chunk_size]\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def get_nodes_from_documents(self, documents: List[Document]) -> List[Node]:\n",
    "        \"\"\"Convert documents to nodes by splitting text into chunks\"\"\"\n",
    "        nodes = []\n",
    "        for doc in documents:\n",
    "            text_chunks = self.split_text(doc.text)\n",
    "            for i, text_chunk in enumerate(text_chunks):\n",
    "                ## Copy metadata and add chunk info:\n",
    "                metadata = doc.metadata.copy()\n",
    "                metadata.update({\n",
    "                    \"chunk_index\": i,\n",
    "                    \"total_chunks\": len(text_chunks),\n",
    "\n",
    "                })\n",
    "                nodes.append(Node(\n",
    "                    text=text_chunk, metadata=metadata))\n",
    "                \n",
    "        return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDirectoryReader:\n",
    "    \"\"\"Read all text files from a directory and return a list of documents.\"\"\"\n",
    "    def __init__(self, directory_path: str) -> None:\n",
    "        self.directory_path = directory_path\n",
    "\n",
    "    def load_data(self) -> List['Document']:\n",
    "        \"\"\" Load all text files from the directory. \"\"\"\n",
    "        documents = []\n",
    "        for filename in os.listdir(self.directory_path):\n",
    "            if filename.endswith('.txt'):\n",
    "                with open(os.path.join(self.directory_path, filename), 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "                documents.append(Document(text, metadata={\"source\": filename}))\n",
    "        return documents\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "## Need to create a base embedding class that extensible for different embedding models: Cohere and HuggingFace.\n",
    "class BaseEmbedding:\n",
    "    def get_embedding(self, text: List[str]) -> List[float]:\n",
    "        \"\"\"Get embedding from a single text file using API.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method.\")\n",
    "\n",
    "class OpenAIEmbedding(BaseEmbedding):\n",
    "    def __init__(self, model_name: str = \"text-embedding-ada-002\") -> None:\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def get_embedding(self, text: List[str]) -> List[float]:\n",
    "        \"\"\"Get embedding from a single text file using OpenAI API.\"\"\"\n",
    "        response = openai.embeddings.create(\n",
    "            model=self.model_name,\n",
    "            input=text,\n",
    "            encoding_format=\"float\",\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    def get_embeddings(self, texts:List[str]) -> List[List[float]]:\n",
    "        \"\"\"Get embeddings for a list of texts.\"\"\"\n",
    "        return [self.get_embedding([text]) for text in texts]\n",
    "    \n",
    "class LLMResponseSynthesizer:\n",
    "    def __init__(self, model_name: str = \"gpt-4o-mini\") -> None:\n",
    "        self.model_name = model_name\n",
    "        self.client = openai.OpenAI()\n",
    "\n",
    "        self.restrictions = \"\"\"\n",
    "        Don't hallucinate\n",
    "        \"\"\"\n",
    "        self.prompt_template = \"\"\"\n",
    "        You are a helpful assistant that can answer questions based on the provided context within the restrictions permitted\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question:\n",
    "        {query}\n",
    "\n",
    "        Restrictions:\n",
    "        {restrictions}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "    def synthesize(self, query: List[str], nodes: List[Node]) -> str:\n",
    "        \"\"\"Synthesize a response from the context and query using the initialized LLM.        \n",
    "        \"\"\"\n",
    "\n",
    "        ## Build context from nodes:\n",
    "        context = \"\\n\\n\".join([f\"Document chunk: {node.text}\" for node in nodes])\n",
    "\n",
    "        ## Build the prompt with context and query:\n",
    "        prompt = self.prompt_template.format(context=context, query=query, restrictions=self.restrictions)\n",
    "        # print(prompt)\n",
    "\n",
    "        ## Call OpenAI API:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }],\n",
    "            \n",
    "        )\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding and Vector Store:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response:\n",
    "    def __init__(self, response: str,) -> None:\n",
    "        self.response = response\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.response\n",
    "\n",
    "class SimpleVectorStore:\n",
    "    \"\"\"Simple Vector Store: Functionalities to add nodes, retrieve nodes based on similarity search (top_k using cosine similarity)\"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self.embeddings = []\n",
    "        self.node_ids = []\n",
    "        self.node_dict = {} ## Store actual node objects by ID\n",
    "\n",
    "    def add_notes(self, nodes: List[Node], embeddings: List[List[float]]) -> None:\n",
    "        for node, embedding in zip(nodes, embeddings):\n",
    "            self.embeddings.append(embedding)\n",
    "            self.node_ids.append(node.node_id)\n",
    "            self.node_dict[node.node_id] = node\n",
    "\n",
    "    def similarity_search(self, query_embedding: List[float], top_k: int = 2) -> List[Node]:\n",
    "        \"\"\"Find `top_k` most similar nodes to the query embedding using cosine similarity.\"\"\"\n",
    "\n",
    "        if not self.embeddings:\n",
    "            logging.warning(\"No embeddings in the vector store.\")\n",
    "            return []\n",
    "        \n",
    "        ## Convert lists to tensor \n",
    "        query_tensor = torch.tensor(query_embedding, dtype=torch.float32)\n",
    "        embeddings_tensor = torch.tensor(self.embeddings, dtype=torch.float32)\n",
    "\n",
    "        ##Normalize the query embedding and c\n",
    "        query_tensor = F.normalize(query_tensor, p=2, dim=0)\n",
    "        embeddings_tensor = F.normalize(embeddings_tensor, p=2, dim=1)\n",
    "\n",
    "        ## Compute cosine similarities:\n",
    "        similarities = torch.matmul(query_tensor, embeddings_tensor.T)\n",
    "\n",
    "        ##Get top_k indices:\n",
    "        top_indices = torch.argsort(similarities, descending=True)[:top_k].tolist()\n",
    "\n",
    "        ## Return the nodes corresponding to the top_k indices:\n",
    "        return [self.node_dict[self.node_ids[idx]] for idx in top_indices]\n",
    "    \n",
    "## Query Engine:\n",
    "class QueryEngine:\n",
    "    def __init__(\n",
    "            self, \n",
    "            vector_store: SimpleVectorStore, \n",
    "            response_synthesizer: LLMResponseSynthesizer, \n",
    "            similarity_topk: int = 2,\n",
    "            ) -> None:\n",
    "        self.vector_store = vector_store\n",
    "        self.response_synthesizer = response_synthesizer\n",
    "        self.embedding_service = OpenAIEmbedding()\n",
    "        self.similarity_topk = similarity_topk\n",
    "\n",
    "    def query(self, query: str) -> Response:\n",
    "        \"\"\"Execute the query and return the response.\"\"\"\n",
    "\n",
    "        ## Get query embedding:\n",
    "        query_embedding = self.embedding_service.get_embedding(query) ## Singe statement so we use `get_embedding`\n",
    "\n",
    "        ## Retrieve the relevant nodes using similarity search:\n",
    "        retrieved_nodes = self.vector_store.similarity_search(\n",
    "            query_embedding=query_embedding,\n",
    "            top_k=self.similarity_topk,\n",
    "        )\n",
    "\n",
    "        ## Generate response\n",
    "        response = self.response_synthesizer.synthesize(query=query, nodes=retrieved_nodes)\n",
    "\n",
    "        return response ## To experiment.\n",
    "        #Response(response=response)\n",
    "    \n",
    "## Vector Store Index:\n",
    "class VectorStoreIndex:\n",
    "    \"\"\"Vector Store Index: Manage nodes, embeddings, and vector store.\"\"\"\n",
    "    def __init__(self, nodes: List[Node], vector_store: SimpleVectorStore, similarity_topk: int = 2) -> None:\n",
    "        self.nodes = nodes\n",
    "        self.vector_store = vector_store\n",
    "        self.similarity_topk = similarity_topk\n",
    "\n",
    "    @classmethod\n",
    "    def from_documents(\n",
    "        cls, \n",
    "        documents: List[Document],\n",
    "        embedding_service: OpenAIEmbedding,\n",
    "        node_parser=None) -> None:\n",
    "\n",
    "        \"\"\" Create index from documents\"\"\"\n",
    "        ## Initialize the embedding service:\n",
    "        embedding_service = embedding_service or OpenAIEmbedding()\n",
    "        node_parser = node_parser or SimpleNodeParser()\n",
    "\n",
    "        ## Create nodes from documents:\n",
    "        nodes = node_parser.get_nodes_from_documents(documents=documents)\n",
    "\n",
    "        ## Get embeddings for all nodes:\n",
    "        texts = [node.text for node in nodes]\n",
    "        embeddings = embedding_service.get_embeddings(texts=texts)\n",
    "\n",
    "        ## Create and populate vector store:\n",
    "        vector_store = SimpleVectorStore()\n",
    "        vector_store.add_notes(nodes=nodes, embeddings=embeddings)\n",
    "\n",
    "        return cls(nodes=nodes, vector_store=vector_store)\n",
    "    \n",
    "    def as_query_engine(self, response_synthesizer: LLMResponseSynthesizer, similarity_topk: int = 2) -> QueryEngine:\n",
    "        \"\"\"Create a query engine from this index\"\"\"\n",
    "        response_synthesizer = response_synthesizer or LLMResponseSynthesizer() ## Default to a simple response synthesizer\n",
    "        return QueryEngine(\n",
    "            vector_store=self.vector_store,\n",
    "            response_synthesizer=response_synthesizer,\n",
    "            similarity_topk=similarity_topk,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"../apps/data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes 8-9s seconds to embed 2 documents.\n",
    "vector_index = VectorStoreIndex.from_documents(documents=[documents[0]], embedding_service=OpenAIEmbedding())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine(\n",
    "    response_synthesizer=LLMResponseSynthesizer(), \n",
    "    similarity_topk=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"This is a LinkedIn profile. Give me the name, position, job history, and location of the individual as json\"\n",
    "response = query_engine.query(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">```json\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"name\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Andrew Ng\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"position\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Founder of DeepLearning.AI; Managing General Partner of AI Fund; Exec Chairman of Landing AI\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"job_history\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"title\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Founder of DeepLearning.AI\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"company\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"DeepLearning.AI\"</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"title\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Managing General Partner\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"company\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"AI Fund\"</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"title\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Exec Chairman\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"company\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Landing AI\"</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"title\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Director\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"company\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Stanford AI Lab\"</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"title\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Adjunct Professor\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"company\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Stanford University, Computer Science Department\"</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "  <span style=\"font-weight: bold\">]</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"location\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Palo Alto, California, United States\"</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "```\n",
       "</pre>\n"
      ],
      "text/plain": [
       "```json\n",
       "\u001b[1m{\u001b[0m\n",
       "  \u001b[32m\"name\"\u001b[0m: \u001b[32m\"Andrew Ng\"\u001b[0m,\n",
       "  \u001b[32m\"position\"\u001b[0m: \u001b[32m\"Founder of DeepLearning.AI; Managing General Partner of AI Fund; Exec Chairman of Landing AI\"\u001b[0m,\n",
       "  \u001b[32m\"job_history\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\n",
       "      \u001b[32m\"title\"\u001b[0m: \u001b[32m\"Founder of DeepLearning.AI\"\u001b[0m,\n",
       "      \u001b[32m\"company\"\u001b[0m: \u001b[32m\"DeepLearning.AI\"\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "      \u001b[32m\"title\"\u001b[0m: \u001b[32m\"Managing General Partner\"\u001b[0m,\n",
       "      \u001b[32m\"company\"\u001b[0m: \u001b[32m\"AI Fund\"\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "      \u001b[32m\"title\"\u001b[0m: \u001b[32m\"Exec Chairman\"\u001b[0m,\n",
       "      \u001b[32m\"company\"\u001b[0m: \u001b[32m\"Landing AI\"\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "      \u001b[32m\"title\"\u001b[0m: \u001b[32m\"Director\"\u001b[0m,\n",
       "      \u001b[32m\"company\"\u001b[0m: \u001b[32m\"Stanford AI Lab\"\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "      \u001b[32m\"title\"\u001b[0m: \u001b[32m\"Adjunct Professor\"\u001b[0m,\n",
       "      \u001b[32m\"company\"\u001b[0m: \u001b[32m\"Stanford University, Computer Science Department\"\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"location\"\u001b[0m: \u001b[32m\"Palo Alto, California, United States\"\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n",
       "```\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Andrew Ng'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'position'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Founder of DeepLearning.AI; Managing General Partner of AI Fund; Exec Chairman of Landing AI'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'job_history'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Founder of DeepLearning.AI'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'company'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'DeepLearning.AI'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Managing General Partner'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'company'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'AI Fund'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Exec Chairman'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'company'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Landing AI'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Director'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'company'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Stanford AI Lab'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Adjunct Professor'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'company'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Stanford University, Computer Science Department'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'location'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Palo Alto, California, United States'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'name'\u001b[0m: \u001b[32m'Andrew Ng'\u001b[0m,\n",
       "    \u001b[32m'position'\u001b[0m: \u001b[32m'Founder of DeepLearning.AI; Managing General Partner of AI Fund; Exec Chairman of Landing AI'\u001b[0m,\n",
       "    \u001b[32m'job_history'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'Founder of DeepLearning.AI'\u001b[0m, \u001b[32m'company'\u001b[0m: \u001b[32m'DeepLearning.AI'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'Managing General Partner'\u001b[0m, \u001b[32m'company'\u001b[0m: \u001b[32m'AI Fund'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'Exec Chairman'\u001b[0m, \u001b[32m'company'\u001b[0m: \u001b[32m'Landing AI'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'Director'\u001b[0m, \u001b[32m'company'\u001b[0m: \u001b[32m'Stanford AI Lab'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'Adjunct Professor'\u001b[0m, \u001b[32m'company'\u001b[0m: \u001b[32m'Stanford University, Computer Science Department'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'location'\u001b[0m: \u001b[32m'Palo Alto, California, United States'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "markdown_text =response.choices[0].message.content\n",
    "# Remove the markdown JSON markers and any extra whitespace\n",
    "json_string = re.sub(r'```json\\n|\\n```', '', markdown_text).strip()\n",
    "\n",
    "# Parse the string into a Python dictionary\n",
    "data = json.loads(json_string)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes 8.5 seconds to embed 1 document.\n",
    "vector_index = VectorStoreIndex.from_documents(documents=[documents[0]], embedding_service=OpenAIEmbedding())\n",
    "query_engine = vector_index.as_query_engine(response_synthesizer=LLMResponseSynthesizer(), similarity_topk=2)\n",
    "query = \"This is a LinkedIn profile. Give me the name for the first person.\"\n",
    "response = query_engine.query(query=query)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes 8.5 seconds to embed 1 document.\n",
    "vector_index = VectorStoreIndex.from_documents(documents=[documents[1]], embedding_service=OpenAIEmbedding())\n",
    "query_engine = vector_index.as_query_engine(response_synthesizer=LLMResponseSynthesizer(), similarity_topk=5)\n",
    "query = \"This is a LinkedIn profile. Give me the name, position, job history - all of them, and location of the individual as json for the first person.\"\n",
    "response = query_engine.query(query=query)\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
