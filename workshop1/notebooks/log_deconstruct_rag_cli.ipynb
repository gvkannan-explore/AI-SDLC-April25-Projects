{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective:\n",
    "- Using the RAG system from HW1, create a simple web app using Gradio and capture the system log for debugging and create a MVE!\n",
    "- Log interaction into a db and csv file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vigneshkannan/Documents/Projects/AI-SDLC-April25-Projects/uv_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import fitz ## PyMuPDF\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "from tqdm.notebook import tqdm\n",
    "import traceback\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Optional, Dict, Any, Union\n",
    "import logging\n",
    "from rich import print\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vigneshkannan/Documents/Projects/AI-SDLC-April25-Projects/workshop1/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_path=\"../../project_secrets.env\")\n",
    "load_dotenv(dotenv_path=\"../../../ai_sdlc_secrets.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path(os.environ.get(\"ROOT_DIR\")) ## type: ignore\n",
    "sys_path = root_dir.parent.parent / \"scripts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESTRICTIONS = \"\"\"\n",
    "Don't hallucinate\n",
    "Don't provide information that is not present in the context. Apologize and request more information if the context is not helpful.\n",
    "Cross-question the user to get more information if the context is not helpful.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a helpful assistant that can answer questions based on the provided context within the restrictions permitted\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Restrictions:\n",
    "{restrictions}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    \"model_name\": \"gpt-4o-mini\",\n",
    "    \"model_provider\": \"openai\",\n",
    "    \"model_parameters\": {\n",
    "        \"max_tokens\": 1000,\n",
    "        \"top_p\": 1,\n",
    "        \"temperature\": 0.3,}\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG without logging - For reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Document and Nodes\n",
    "class Document:\n",
    "    def __init__(self, text: str, metadata: Optional[Dict[str, Any]] = None):\n",
    "        self.text = text\n",
    "        self.metadata = metadata or {}\n",
    "\n",
    "class Node:\n",
    "    def __init__(\n",
    "            self, \n",
    "            text: str, \n",
    "            metadata: Optional[Dict[str, Any]] = None,\n",
    "            node_id: Optional[str] = None):\n",
    "        self.text = text\n",
    "        self.metadata = metadata or {}\n",
    "        self.node_id = node_id or f\"node_{id(self)}\" ## Simple Unique ID if not provided\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Node(id={self.node_id}, text={self.text[:50]}, metadata={self.metadata})\"\n",
    "    \n",
    "\n",
    "class Response:\n",
    "    def __init__(self, response: str, prompt: str) -> None:\n",
    "        self.response = response\n",
    "        self.prompt = prompt\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.response\n",
    "    \n",
    "## 3. For Indexing      \n",
    "class SimpleNodeParser:\n",
    "    def __init__(self, chunk_size: int = 4096, chunk_overlap: int= 200) -> None:\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into chunks with overalap\"\"\"\n",
    "        if len(text) <= self.chunk_size:\n",
    "            return [text]\n",
    "        \n",
    "        chunks = []\n",
    "        for i in range(0, len(text), self.chunk_size - self.chunk_overlap):\n",
    "            chunk = text[i:i+self.chunk_size]\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def get_nodes_from_documents(self, documents: List[Document]) -> List[Node]:\n",
    "        \"\"\"Convert documents to nodes by splitting text into chunks\"\"\"\n",
    "        nodes = []\n",
    "        for doc in documents:\n",
    "            text_chunks = self.split_text(doc.text)\n",
    "            for i, text_chunk in enumerate(text_chunks):\n",
    "                ## Copy metadata and add chunk info:\n",
    "                metadata = doc.metadata.copy()\n",
    "                metadata.update({\n",
    "                    \"chunk_index\": i,\n",
    "                    \"total_chunks\": len(text_chunks),\n",
    "\n",
    "                })\n",
    "                nodes.append(Node(\n",
    "                    text=text_chunk, metadata=metadata))\n",
    "                \n",
    "        return nodes\n",
    "    \n",
    "class SimpleDirectoryReader:\n",
    "    \"\"\"Read all text files from a directory and return a list of documents.\"\"\"\n",
    "    def __init__(self, directory_path: str) -> None:\n",
    "        self.directory_path = directory_path\n",
    "\n",
    "    def load_data(self) -> List['Document']:\n",
    "        \"\"\" Load all text files from the directory. \"\"\"\n",
    "        documents = []\n",
    "        for filename in os.listdir(self.directory_path):\n",
    "            if filename.endswith('.txt'):\n",
    "                with open(os.path.join(self.directory_path, filename), 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "                documents.append(Document(text, metadata={\"source\": filename}))\n",
    "        return documents\n",
    "\n",
    "import openai\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "## Need to create a base embedding class that extensible for different embedding models: Cohere and HuggingFace.\n",
    "class BaseEmbedding:\n",
    "    def get_embedding(self, text: List[str]) -> List[float]:\n",
    "        \"\"\"Get embedding from a single text file using API.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method.\")\n",
    "\n",
    "class OpenAIEmbedding(BaseEmbedding):\n",
    "    def __init__(self, model_name: str = \"text-embedding-ada-002\") -> None:\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def get_embedding(self, text: str ) -> List[float]:\n",
    "        \"\"\"Get embedding from a single text file using OpenAI API.\"\"\"\n",
    "        response = openai.embeddings.create(\n",
    "            model=self.model_name,\n",
    "            input=text,\n",
    "            encoding_format=\"float\",\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    def get_embeddings(self, texts:List[str]) -> List[List[float]]:\n",
    "        \"\"\"Get embeddings for a list of texts.\"\"\"\n",
    "        return [self.get_embedding([text]) for text in texts]\n",
    "    \n",
    "class LLMResponseSynthesizer:\n",
    "    def __init__(self, restrictions: str = RESTRICTIONS, prompt_template: str = PROMPT_TEMPLATE, model_config: Dict[str, Any] = MODEL_CONFIG) -> None:\n",
    "        self.client = openai.OpenAI() ## Need to update this to support different LLM providers.\n",
    "        self.model_config = model_config\n",
    "\n",
    "        self.restrictions = restrictions\n",
    "        self.prompt_template = prompt_template\n",
    "\n",
    "    def synthesize(self, query: str, nodes: List[Node]) -> Response:\n",
    "        \"\"\"Synthesize a response from the context and query using the initialized LLM.        \n",
    "        \"\"\"\n",
    "\n",
    "        ## Build context from nodes:\n",
    "        context = \"\\n\\n\".join([f\"Document chunk: {node.text}\" for node in nodes])\n",
    "\n",
    "        ## Build the prompt with context and query:\n",
    "        prompt = self.prompt_template.format(context=context, query=query, restrictions=self.restrictions)\n",
    "        # print(prompt)\n",
    "\n",
    "        ## Call OpenAI API:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_config['model_name'],\n",
    "            max_tokens=self.model_config['model_parameters']['max_tokens'],\n",
    "            top_p=self.model_config['model_parameters']['top_p'],\n",
    "            temperature=self.model_config['model_parameters']['temperature'],\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }],\n",
    "        )\n",
    "\n",
    "        return Response(response=response, prompt=prompt) # type: ignore\n",
    "\n",
    "\n",
    "class SimpleVectorStore:\n",
    "    \"\"\"Simple Vector Store: Functionalities to add nodes, retrieve nodes based on similarity search (top_k using cosine similarity)\"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self.embeddings = []\n",
    "        self.node_ids = []\n",
    "        self.node_dict = {} ## Store actual node objects by ID\n",
    "\n",
    "    def add_notes(self, nodes: List[Node], embeddings: List[List[float]]) -> None:\n",
    "        for node, embedding in zip(nodes, embeddings):\n",
    "            self.embeddings.append(embedding)\n",
    "            self.node_ids.append(node.node_id)\n",
    "            self.node_dict[node.node_id] = node\n",
    "\n",
    "    def similarity_search(self, query_embedding: List[float], top_k: int = 2) -> List[Node]:\n",
    "        \"\"\"Find `top_k` most similar nodes to the query embedding using cosine similarity.\"\"\"\n",
    "\n",
    "        if not self.embeddings:\n",
    "            logging.warning(\"No embeddings in the vector store.\")\n",
    "            return []\n",
    "        \n",
    "        ## Convert lists to tensor \n",
    "        query_tensor = torch.tensor(query_embedding, dtype=torch.float32)\n",
    "        embeddings_tensor = torch.tensor(self.embeddings, dtype=torch.float32)\n",
    "\n",
    "        ##Normalize the query embedding and c\n",
    "        query_tensor = F.normalize(query_tensor, p=2, dim=0)\n",
    "        embeddings_tensor = F.normalize(embeddings_tensor, p=2, dim=1)\n",
    "\n",
    "        ## Compute cosine similarities:\n",
    "        similarities = torch.matmul(query_tensor, embeddings_tensor.T)\n",
    "\n",
    "        ##Get top_k indices:\n",
    "        top_indices = torch.argsort(similarities, descending=True)[:top_k].tolist()\n",
    "\n",
    "        ## Return the nodes corresponding to the top_k indices:\n",
    "        return [self.node_dict[self.node_ids[idx]] for idx in top_indices]\n",
    "    \n",
    "## Query Engine:\n",
    "class QueryEngine:\n",
    "    def __init__(\n",
    "            self, \n",
    "            vector_store: SimpleVectorStore, \n",
    "            response_synthesizer: LLMResponseSynthesizer, \n",
    "            similarity_topk: int = 2,\n",
    "            ) -> None:\n",
    "        self.vector_store = vector_store\n",
    "        self.response_synthesizer = response_synthesizer\n",
    "        self.embedding_service = OpenAIEmbedding()\n",
    "        self.similarity_topk = similarity_topk\n",
    "\n",
    "    def query(self, query: str) -> Response:\n",
    "        \"\"\"Execute the query and return the response.\"\"\"\n",
    "\n",
    "        ## Get query embedding:\n",
    "        query_embedding = self.embedding_service.get_embedding(query) # type: ignore ## Single statement so we use `get_embedding`\n",
    "\n",
    "        ## Retrieve the relevant nodes using similarity search:\n",
    "        retrieved_nodes = self.vector_store.similarity_search(\n",
    "            query_embedding=query_embedding,\n",
    "            top_k=self.similarity_topk,\n",
    "        )\n",
    "\n",
    "        ## Generate response\n",
    "        response = self.response_synthesizer.synthesize(query=query, nodes=retrieved_nodes)\n",
    "\n",
    "        return response\n",
    "    \n",
    "## Vector Store Index:\n",
    "class VectorStoreIndex:\n",
    "    \"\"\"Vector Store Index: Manage nodes, embeddings, and vector store.\"\"\"\n",
    "    def __init__(self, nodes: List[Node], vector_store: SimpleVectorStore, similarity_topk: int = 2) -> None:\n",
    "        self.nodes = nodes\n",
    "        self.vector_store = vector_store\n",
    "        self.similarity_topk = similarity_topk\n",
    "\n",
    "    @classmethod\n",
    "    def from_documents(\n",
    "        cls, \n",
    "        documents: List[Document],\n",
    "        embedding_service: OpenAIEmbedding,\n",
    "        node_parser=None) -> None:\n",
    "\n",
    "        \"\"\" Create index from documents\"\"\"\n",
    "        ## Initialize the embedding service:\n",
    "        embedding_service = embedding_service or OpenAIEmbedding()\n",
    "        node_parser = node_parser or SimpleNodeParser()\n",
    "\n",
    "        ## Create nodes from documents:\n",
    "        nodes = node_parser.get_nodes_from_documents(documents=documents)\n",
    "\n",
    "        ## Get embeddings for all nodes:\n",
    "        texts = [node.text for node in nodes]\n",
    "        embeddings = embedding_service.get_embeddings(texts=texts)\n",
    "\n",
    "        ## Create and populate vector store:\n",
    "        vector_store = SimpleVectorStore()\n",
    "        vector_store.add_notes(nodes=nodes, embeddings=embeddings)\n",
    "\n",
    "        return cls(nodes=nodes, vector_store=vector_store)\n",
    "    \n",
    "    def as_query_engine(self, response_synthesizer: LLMResponseSynthesizer, similarity_topk: int = 2) -> QueryEngine:\n",
    "        \"\"\"Create a query engine from this index\"\"\"\n",
    "        response_synthesizer = response_synthesizer or LLMResponseSynthesizer() ## Default to a simple response synthesizer\n",
    "        return QueryEngine(\n",
    "            vector_store=self.vector_store,\n",
    "            response_synthesizer=response_synthesizer,\n",
    "            similarity_topk=similarity_topk,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding and Vector Store:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configure logging to print to console.\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path(os.environ.get(\"ROOT_DIR\"))\n",
    "log_dir = root_dir / \"logs\"\n",
    "log_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified RAG-System with logging capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGInteractionLogger:\n",
    "    \"\"\"Handles logging of RAG system interactions to both SQLite and CSV\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = \"rag_interactions.db\", csv_path: str = \"rag_interactions.csv\"):\n",
    "        self.db_path = db_path\n",
    "        self.csv_path = csv_path\n",
    "        self._init_db()\n",
    "\n",
    "    def _init_db(self) -> None:\n",
    "        \"\"\"Initialize SQLite database with required tables\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        c = conn.cursor()\n",
    "        \n",
    "        # Create interactions table with enhanced fields\n",
    "        c.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS interactions (\n",
    "                id TEXT PRIMARY KEY,\n",
    "                timestamp TEXT,\n",
    "                query TEXT,\n",
    "                response TEXT,\n",
    "                source_document TEXT,\n",
    "                system_prompt TEXT,\n",
    "                model_name TEXT,\n",
    "                model_type TEXT,\n",
    "                model_parameters TEXT,\n",
    "                retrieved_context TEXT,\n",
    "                processing_time REAL,\n",
    "                metadata TEXT\n",
    "                \n",
    "            )\n",
    "        ''')\n",
    "        logger.info(f\"Created interactions table in {self.db_path}\")\n",
    "        \n",
    "        # Create system_logs table\n",
    "        c.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS system_logs (\n",
    "                id TEXT PRIMARY KEY,\n",
    "                timestamp TEXT,\n",
    "                level TEXT,\n",
    "                message TEXT,\n",
    "                module TEXT,\n",
    "                function TEXT,\n",
    "                traceback TEXT,\n",
    "                llm_config TEXT\n",
    "            )\n",
    "        ''')\n",
    "        logger.info(f\"Created system_logs table in {self.db_path}\")\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        return None\n",
    "    \n",
    "    def log_system_event(\n",
    "            self, \n",
    "            level: str,\n",
    "            message: str,\n",
    "            module: str,\n",
    "            function: str,\n",
    "            traceback: str,\n",
    "            llm_config: Optional[Dict[str, Any]] = None,\n",
    "            ) -> None:\n",
    "        \n",
    "        \"\"\"Log system events to database with LLM configuration when needed\"\"\"\n",
    "        log_id = str(uuid.uuid4())\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        llm_config_str = json.dumps(llm_config or {})\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        c = conn.cursor()\n",
    "        c.execute('''\n",
    "            INSERT INTO syste_logs\n",
    "            (id, timestamp, level, message, module, function, lin_number, llm_config)\n",
    "            VALUES (? ? ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (log_id, timestamp, level, message, module, function, traceback, llm_config_str))\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        return None\n",
    "    \n",
    "    def log_interaction(self, \n",
    "                       query: str, \n",
    "                       response: str, \n",
    "                       source_document: str,\n",
    "                       system_prompt: str,\n",
    "                       model_name: str,\n",
    "                       model_type: str,\n",
    "                       model_parameters: Dict[str, Any],\n",
    "                       retrieved_context: List[str],\n",
    "                       metadata: Optional[Dict[str, Any]] = None,\n",
    "                       processing_time: float = 0.0) -> None:\n",
    "        \n",
    "        \"\"\"Log an interaction to both database and CSV with enhanced LLM details\"\"\"\n",
    "        interaction_id = str(uuid.uuid4())\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        metadata_str = json.dumps(metadata or {})\n",
    "        model_params_str = json.dumps(model_parameters)\n",
    "        context_str = json.dumps(retrieved_context)\n",
    "        \n",
    "        # Log to SQLite\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        c = conn.cursor()\n",
    "        c.execute('''\n",
    "            INSERT INTO interactions \n",
    "            (id, timestamp, query, response, source_document, \n",
    "             system_prompt, model_name, model_type, model_parameters,\n",
    "             retrieved_context, processing_time, metadata)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (interaction_id, timestamp, query, response, source_document,\n",
    "              system_prompt, model_name, model_type, model_params_str,\n",
    "              context_str, processing_time, metadata_str))\n",
    "        conn.commit()\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGwithLogging:\n",
    "    def __init__(self, data_dir: str, model_config: Dict[str, Any], similarity_topk: int = 2, restrictions: str = RESTRICTIONS, prompt_template: str = PROMPT_TEMPLATE):\n",
    "        self.data_dir = data_dir\n",
    "        self.similarity_topk = similarity_topk\n",
    "        self.model_config = model_config\n",
    "        self.restrictions = restrictions\n",
    "        self.prompt_template = prompt_template\n",
    "        self.logger = RAGInteractionLogger()\n",
    "        self.documents = self._load_documents()\n",
    "        self.vector_index = self.create_vector_index()\n",
    "        self.query_engine = self._create_query_engine() ## Fundctions not meant to be called directly.\n",
    "\n",
    "    def _load_documents(self) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load documents from directory and log the process.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            documents = []\n",
    "            for filename in os.listdir(self.data_dir)[:3]:\n",
    "                if filename.endswith('.txt'):\n",
    "                    file_path = os.path.join(self.data_dir, filename)\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        text = file.read()\n",
    "                    documents.append(Document(text, metadata={\"source\": filename}))\n",
    "            return documents\n",
    "        except Exception as e: \n",
    "            self.logger.log_system_event(\n",
    "                level=\"ERROR\",\n",
    "                message=f\"Error loading documents: {str(e)}\",\n",
    "                module=__name__,\n",
    "                function=\"_load_documents\",\n",
    "                traceback=traceback.format_exc()\n",
    "            )\n",
    "            raise Exception(f\"Error loading documents: {str(e)}\")\n",
    "        \n",
    "    def create_vector_index(self) -> VectorStoreIndex:\n",
    "        \"\"\"Create vector index from documents and log the process\"\"\"\n",
    "        try:\n",
    "            node_parser = SimpleNodeParser()\n",
    "            embedding_service = OpenAIEmbedding()\n",
    "            nodes = node_parser.get_nodes_from_documents(self.documents)\n",
    "            texts = [node.text for node in nodes]\n",
    "            embeddings = embedding_service.get_embeddings(texts)\n",
    "            \n",
    "            vector_store = SimpleVectorStore()\n",
    "            vector_store.add_notes(nodes=nodes, embeddings=embeddings)\n",
    "\n",
    "            return VectorStoreIndex(nodes=nodes, vector_store=vector_store, similarity_topk=self.similarity_topk)\n",
    "        except Exception as e:\n",
    "            self.logger.log_system_event(\n",
    "                level=\"ERROR\",\n",
    "                message=f\"Error creating vector index: {str(e)}\",\n",
    "                module=__name__,\n",
    "                function=\"_create_vector_index\",\n",
    "                traceback=traceback.format_exc()\n",
    "            )   \n",
    "            raise Exception(f\"Error creating vector index: {str(e)}\")\n",
    "        \n",
    "    def _create_query_engine(self) -> QueryEngine:\n",
    "        \"\"\"Create query engine from vector index and log the process\"\"\"\n",
    "        try:\n",
    "            response_synthesizer = LLMResponseSynthesizer(model_config=self.model_config, restrictions=self.restrictions, prompt_template=self.prompt_template)\n",
    "            return self.vector_index.as_query_engine(response_synthesizer=response_synthesizer, similarity_topk=self.similarity_topk)\n",
    "        except Exception as e:\n",
    "            self.logger.log_system_event(\n",
    "                level=\"ERROR\",\n",
    "                message=f\"Error creating query engine: {str(e)}\",\n",
    "                module=__name__,\n",
    "                function=\"_create_query_engine\",\n",
    "                traceback=traceback.format_exc()\n",
    "            )\n",
    "            raise Exception(f\"Error creating query engine: {str(e)}\")\n",
    "        \n",
    "    def query(self, query: str, model_config: Dict[str, Any]) -> Union[str, Response]:\n",
    "        \"\"\"Execute query with enhanced logging\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        try:\n",
    "            # Get the response and context\n",
    "            response = self.query_engine.query(query)\n",
    "            query_embedding = self.query_engine.embedding_service.get_embedding(query)\n",
    "            retrieved_nodes = self.query_engine.vector_store.similarity_search(\n",
    "                query_embedding=query_embedding,\n",
    "                top_k=self.query_engine.similarity_topk\n",
    "            )\n",
    "            retrieved_context = [node.text for node in retrieved_nodes]\n",
    "            processing_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "            self.logger.log_interaction(\n",
    "                query=query,\n",
    "                response=str(response.response),\n",
    "                source_document=self.documents[0].metadata.get(\"source\", \"unknown\"),\n",
    "                system_prompt=response.prompt,\n",
    "                model_name=self.model_config['model_name'],\n",
    "                model_type=self.model_config['model_provider'],\n",
    "                model_parameters=self.model_config['model_parameters'],\n",
    "                retrieved_context=retrieved_context,\n",
    "                metadata={\"processing_time\": processing_time},\n",
    "                processing_time=processing_time\n",
    "            )\n",
    "            return response.response\n",
    "\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_system_event(\n",
    "                level=\"ERROR\",\n",
    "                message=f\"Error executing query: {str(e)}\",\n",
    "                module=__name__,\n",
    "                function=\"_query\",\n",
    "                traceback=traceback.format_exc()\n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 17:16:41,406 - __main__ - INFO - Created interactions table in rag_interactions.db\n",
      "2025-04-30 17:16:41,407 - __main__ - INFO - Created system_logs table in rag_interactions.db\n",
      "2025-04-30 17:16:42,064 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-30 17:16:42,435 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-30 17:16:42,898 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-30 17:16:43,127 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-30 17:16:43,715 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-30 17:16:44,063 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-30 17:16:44,328 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-30 17:16:44,506 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-30 17:16:44,841 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-30 17:16:45,102 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-30 17:16:45,454 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "rag_sys = RAGwithLogging(data_dir=\"../apps/data\", model_config=MODEL_CONFIG, similarity_topk=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 17:16:47,613 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-30 17:16:48,273 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-30 17:16:48,430 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BSA1PpH8UMNZvWfqysr6Vaobm4e0l', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The name of the first person in the document is Chris Griffin.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746051407, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_0392822090', usage=CompletionUsage(completion_tokens=14, prompt_tokens=2298, total_tokens=2312, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_sys.query(query=\"What is the name of the first person in the document?\", model_config=MODEL_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Could not query <span style=\"color: #008000; text-decoration-color: #008000\">'interactions'</span> table: Execution failed on sql <span style=\"color: #008000; text-decoration-color: #008000\">'SELECT * FROM interactions'</span>: no such table: \n",
       "interactions\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Could not query \u001b[32m'interactions'\u001b[0m table: Execution failed on sql \u001b[32m'SELECT * FROM interactions'\u001b[0m: no such table: \n",
       "interactions\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "db_path = log_dir / \"rag_interactions.db\" \n",
    "conn = sqlite3.connect(db_path)\n",
    "try:\n",
    "    interactions_df = pd.read_sql_query(\"SELECT * FROM interactions\", conn)\n",
    "    display(interactions_df) # Use display() for better rendering in Jupyter\n",
    "except pd.io.sql.DatabaseError as e:\n",
    "    print(f\"Could not query 'interactions' table: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = root_dir / \"logs\"\n",
    "db_path = log_dir / \"rag_interactions.db\" \n",
    "conn = sqlite3.connect(db_path)\n",
    "interactions_df = pd.read_sql_query(\"SELECT * FROM interactions\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " interactions_df = pd.read_sql_query(\"SELECT * FROM interactions\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
