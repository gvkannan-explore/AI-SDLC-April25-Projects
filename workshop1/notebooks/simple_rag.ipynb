{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader \n",
    "from pathlib import PosixPath\n",
    "from typing import Union\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.llms.anthropic import Anthropic\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "import sys\n",
    "# from rich import print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions for the future:\n",
    "1. Use ollama for models instead of API calls.\n",
    "    -> Docs mention huggingface.\n",
    "2. Semantic (using LLMs itself) vs Key-word matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If we wanted to monitor the API calls and responses within llama-index,  uncomment this section\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path=\"../../project_secrets.env\")\n",
    "load_dotenv(dotenv_path=\"../../../ai_sdlc_secrets.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path(os.getenv(\"ROOT_DIR\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestRAG:\n",
    "    def __init__(self, data_dir: Union[str, PosixPath]):\n",
    "        self.data_dir = data_dir\n",
    "        self.documents = SimpleDirectoryReader(input_dir=self.data_dir).load_data() ## Similar to pd.read_csv()\n",
    "        self.index = VectorStoreIndex.from_documents(self.documents, ) ## Uses open-ai-embeddings so fails without the API key.\n",
    "        self.query_engine = self.index.as_query_engine(llm=None, similarity_top_k=3)\n",
    "\n",
    "    def query(self, query: str) -> str:\n",
    "        response = self.query_engine.query(query)\n",
    "        return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GeminiConfig:\n",
    "    model_name: str = \"gemini-2.0-flash\"\n",
    "    temperature: float = 0.7\n",
    "    max_tokens: int = 512\n",
    "\n",
    "@dataclass\n",
    "class ClaudeConfig:\n",
    "    model: str = 'claude-3-7-sonnet-latest'\n",
    "    temperature: float = 0.1\n",
    "    max_tokens: int = 512 \n",
    "\n",
    "class RAG_Pipeline:\n",
    "    \"\"\"\n",
    "    A simple RAG pipeline that uses the LlamaIndex library to create a vector store index from documents in a directory and allows querying from index using configured LLM.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir: Union[str, PosixPath], llm_provider: str = \"GoogleGenAI\", llm_config: GeminiConfig = GeminiConfig()):\n",
    "        self.data_dir = data_dir\n",
    "        self.documents = SimpleDirectoryReader(input_dir=self.data_dir).load_data() ## Similar to pd.read_csv()\n",
    "        self.index = VectorStoreIndex.from_documents(self.documents, ) ## Uses open-ai-embeddings so fails without the API key.\n",
    "        \n",
    "        if llm_provider == \"GoogleGenAI\":\n",
    "            self.llm_cfg = llm_config\n",
    "            self.llm = GoogleGenAI(\n",
    "                model=self.llm_cfg.model_name, \n",
    "                temperature=self.llm_cfg.temperature, \n",
    "                max_tokens=self.llm_cfg.max_tokens,) \n",
    "            \n",
    "        elif llm_provider == \"Claude\":\n",
    "            self.llm_cfg = llm_config\n",
    "            self.llm = Anthropic(\n",
    "                model=self.llm_cfg.model,\n",
    "                temperature=self.llm_cfg.temperature,\n",
    "                max_tokens=self.llm_cfg.max_tokens,)\n",
    "        else:\n",
    "            raise Exception(f\"Invalid LLM provided: {llm_provider}. Supported LLMs are: `GoogleGenAI`\")\n",
    "\n",
    "        self.query_engine = self.index.as_query_engine(llm=self.llm)\n",
    "\n",
    "    def query(self, query: str) -> str:\n",
    "        response = self.query_engine.query(query)\n",
    "        return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline = RAG_Pipeline(\n",
    "    data_dir=root_dir, \n",
    "    llm_provider=\"GoogleGenAI\", \n",
    "    llm_config=GeminiConfig())\n",
    "\n",
    "# rag_pipeline = RAG_Pipeline(\n",
    "#     data_dir=root_dir, \n",
    "#     llm_provider=\"Claude\", \n",
    "#     llm_config=ClaudeConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_pipeline.query(\"This is a LinkedIn profile. Give me the name, position, job history, and location of the individual as json\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
